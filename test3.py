import os
import sounddevice as sd
import numpy as np
import queue
import tempfile
import soundfile as sf
from pyannote.audio import Pipeline
import torch
import whisper  # ë˜ëŠ” faster_whisper
from datetime import timedelta


# Hugging Face í† í°ì„ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
# ì´ëŠ” ì½”ë“œì— ë¯¼ê°í•œ ì •ë³´ë¥¼ ì§ì ‘ ë…¸ì¶œí•˜ì§€ ì•ŠëŠ” ì•ˆì „í•œ ë°©ë²•ì…ë‹ˆë‹¤.
HF_TOKEN = os.environ["HF_TOKEN"]
if not HF_TOKEN:
    raise ValueError(
        "Hugging Face ì¸ì¦ í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤. 'HF_TOKEN' í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
    )
# Hugging Face Token í•„ìš”
diarization_pipeline = Pipeline.from_pretrained(
	"pyannote/speaker-diarization-3.1", use_auth_token=HF_TOKEN)

# Whisper ëª¨ë¸ ë¡œë”© (base or small ì¶”ì²œ)
stt_model = whisper.load_model("base")  # ë˜ëŠ” faster-whisper ì‚¬ìš©

# ë§ˆì´í¬ ì„¤ì •
samplerate = 16000
block_duration = 3  # ì´ˆ
q = queue.Queue()

# ì½œë°±


def callback(indata, frames, time, status):
	q.put(indata.copy())

# ì‹œê°„ í¬ë§·


def format_time(seconds):
	return str(timedelta(seconds=round(seconds, 1)))[:-3]


print("ğŸ™ï¸ ì‹¤ì‹œê°„ í™”ì ë¶„ë¦¬ + ìŒì„± ì¸ì‹ ìë§‰ ì‹œì‘ (Ctrl+C ì¢…ë£Œ)")

with sd.InputStream(samplerate=samplerate, channels=1, callback=callback):
	try:
		while True:
			# 3ì´ˆ ë²„í¼ ìˆ˜ì§‘
			frames = []
			for _ in range(int((block_duration * samplerate) / 1024)):
				frames.append(q.get())
			audio_block = np.concatenate(frames, axis=0)

			waveform = torch.from_numpy(
				audio_block.astype(np.float32).T)
			diarization_input = {"waveform": waveform, "sample_rate": samplerate}

			# (1) í™”ì ë¶„ë¦¬
			diarization = diarization_pipeline(diarization_input)

			# (2) ìŒì„± ì¸ì‹
			transcription = stt_model.transcribe(audio_block, batch_size=16)

			# (3) ë§¤í•‘ ë° ìë§‰ ì¶œë ¥
			print("\n--- ğŸ—£ï¸ ì‹¤ì‹œê°„ ìë§‰ ---")
			for turn, _, speaker in diarization.itertracks(yield_label=True):
				seg_start = turn.start
				seg_end = turn.end

				# ì´ êµ¬ê°„ì— í¬í•¨ë˜ëŠ” STT í…ìŠ¤íŠ¸ ì¶”ì¶œ
				seg_texts = [
					segment["text"].strip()
					for segment in transcription["segments"]
					if segment["start"] >= seg_start and segment["end"] <= seg_end
				]

				full_text = " ".join(seg_texts) if seg_texts else "(ìŒì„± ì—†ìŒ)"
				print(
					f"[{format_time(seg_start)} - {format_time(seg_end)}] {speaker}: {full_text}")

	except KeyboardInterrupt:
		print("ğŸ›‘ ì¢…ë£Œë¨.")
